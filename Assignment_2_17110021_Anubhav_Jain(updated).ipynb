{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-2-17110021-Anubhav_Jain.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLe1n79DD7Xk",
        "colab_type": "text"
      },
      "source": [
        "# Neural Approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuRad8CScu8p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10fec8dc-1b2b-469c-c385-e3ea5f32949b"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import string\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import random\n",
        "import re\n",
        "import math\n",
        "import nltk\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rRTm1CFzpzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fp = open(\"/content/drive/My Drive/Theory_of_Computation/speeches.txt\", 'r', encoding=\"utf-8-sig\", errors='ignore')\n",
        "data = fp.read()\n",
        "\n",
        "data = data.replace(\"\\n\", \"\")\n",
        "data = data.replace(\"...\", \". \")\n",
        "data = re.sub(r\"SPEECH [0-9]\",\"\", data)\n",
        "data = re.sub(r\"[:;]\",\".\",data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbSDz5I90qBW",
        "colab_type": "code",
        "outputId": "26664d65-4ec3-4810-f424-d797e0a75803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CsjyJK21eyK",
        "colab_type": "code",
        "outputId": "999cb727-4eb5-4c64-8dc8-7aac76fc83ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruWTvV_XzsRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_tokenize_list = sent_tokenize(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_1IUa1MzrWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = list()\n",
        "for i in range(len(sent_tokenize_list)):\n",
        "  if(len(sent_tokenize_list[i])>1):\n",
        "    lines.append(\"<s> \" + sent_tokenize_list[i][:-1] + \" </s>\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh4u0LgUzzOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.shuffle(lines)\n",
        "cut = int(0.8*len(lines))\n",
        "train = np.array(lines[:cut])\n",
        "test = np.array(lines[cut:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BLpcCDxz50e",
        "colab_type": "code",
        "outputId": "b42b7207-bba6-4c69-9e3a-715339cefb0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "train"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['<s> Six people were killed trying to get him back </s>',\n",
              "       '<s> WE HAVE A GROUP OF PEOPLE THAT WANT TO GET ELECTED WHO HAVE A RIGGED SYSTEM AND WHO HAVE REALLY RIGGED THE SYSTEM TO A LARGE EXTENT </s>',\n",
              "       '<s> THESE ARE AMATEURS </s>', ..., '<s> He’s at two </s>',\n",
              "       '<s> It’s just make America great again </s>',\n",
              "       '<s> Thousands and thousands of people standing outside of an airplane hangar because the hangar was packed </s>'],\n",
              "      dtype='<U1968')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZaUZuae1ntx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_doc(doc):\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  text = doc.translate(table)\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ez_tk4q99DwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = list()\n",
        "seq_length = 2\n",
        "\n",
        "for i in range(len(train)):\n",
        "#   tokens = [\"<start>\"]\n",
        "  tokens = (clean_doc(train[i]))\n",
        "#   tokens.append(\"<end>\")\n",
        "  if(len(tokens)>seq_length):\n",
        "    for k in range(0,len(tokens)-seq_length):\n",
        "      sequences.append(tokens[k:k+seq_length+1])\n",
        "#   titles[i] = ' '.join(clean_doc(titles[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qly5V0Su9Zbf",
        "colab_type": "code",
        "outputId": "8fbc0096-0429-486d-c210-32d3014473a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "sequences[:10]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['s', 'six', 'people'],\n",
              " ['six', 'people', 'were'],\n",
              " ['people', 'were', 'killed'],\n",
              " ['were', 'killed', 'trying'],\n",
              " ['killed', 'trying', 'to'],\n",
              " ['trying', 'to', 'get'],\n",
              " ['to', 'get', 'him'],\n",
              " ['get', 'him', 'back'],\n",
              " ['him', 'back', 's'],\n",
              " ['s', 'we', 'have']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS4-AZIE9dMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sequences)\n",
        "sequences_intEncoded = tokenizer.texts_to_sequences(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KoCQoUD9fIa",
        "colab_type": "code",
        "outputId": "c164950f-7dd6-46fa-826f-922e3ec36a78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PboGcvaQ9g-B",
        "colab_type": "code",
        "outputId": "761b9950-1043-4833-cf63-6c18cce60b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences_intEncoded[:5]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 510, 21], [510, 21, 68], [21, 68, 400], [68, 400, 411], [400, 411, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0MYlZk69jXW",
        "colab_type": "code",
        "outputId": "af9cf19e-586b-4f3f-d7ab-feb3d72f2380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sequences_intEncoded = np.array(sequences_intEncoded)\n",
        "np.shape(sequences)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(136745, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14r-TiYp9lxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = sequences_intEncoded[:,:-1]\n",
        "y = sequences_intEncoded[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6huj7i8N9oUQ",
        "colab_type": "code",
        "outputId": "17a2fc7e-1834-423e-8382-8477623425f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "X[:5]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  1, 510],\n",
              "       [510,  21],\n",
              "       [ 21,  68],\n",
              "       [ 68, 400],\n",
              "       [400, 411]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc7U-ilq9qg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQqQf-dFIlds",
        "colab_type": "code",
        "outputId": "39c59a17-8764-4b62-b649-72e778fb4914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "###### VANILLA RECURRENT NEURAL NETWORK MODEL ######\n",
        "from keras.layers import SimpleRNN\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model1.add(SimpleRNN(25, return_sequences=True))\n",
        "model1.add(SimpleRNN(25))\n",
        "model1.add(Dense(50, activation='relu'))\n",
        "model1.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model1.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 2, 50)             379700    \n",
            "_________________________________________________________________\n",
            "simple_rnn_1 (SimpleRNN)     (None, 2, 25)             1900      \n",
            "_________________________________________________________________\n",
            "simple_rnn_2 (SimpleRNN)     (None, 25)                1275      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                1300      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 7594)              387294    \n",
            "=================================================================\n",
            "Total params: 771,469\n",
            "Trainable params: 771,469\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNiaM08f9tsF",
        "colab_type": "code",
        "outputId": "7be49fd8-720d-4f1a-ba7b-c75e8ad3d342",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "###### LONG SHORT TERM MEMORY NEURAL NETWORK MODEL ######\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(25, return_sequences=True))\n",
        "model.add(LSTM(25))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 2, 50)             379700    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 2, 25)             7600      \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 25)                5100      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 50)                1300      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 7594)              387294    \n",
            "=================================================================\n",
            "Total params: 780,994\n",
            "Trainable params: 780,994\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znxqQqge9vh3",
        "colab_type": "code",
        "outputId": "57f6174f-7cee-4ffb-bd66-33624e8b9c58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        }
      },
      "source": [
        "###### TRAINING LONG SHORT TERM MEMORY NEURAL NETWORK MODEL ######\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, batch_size=128, epochs=40)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Epoch 1/40\n",
            "136745/136745 [==============================] - 64s 469us/step - loss: 6.2121 - acc: 0.0913\n",
            "Epoch 2/40\n",
            "136745/136745 [==============================] - 62s 454us/step - loss: 5.7634 - acc: 0.1022\n",
            "Epoch 3/40\n",
            "136745/136745 [==============================] - 62s 452us/step - loss: 5.3309 - acc: 0.1487\n",
            "Epoch 4/40\n",
            "  4352/136745 [..............................] - ETA: 59s - loss: 5.1072 - acc: 0.1691"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a8415f86625d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc5giR59-1q-",
        "colab_type": "code",
        "outputId": "2efd97ba-cf16-4732-d077-437a02b9a92f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        }
      },
      "source": [
        "###### TRAINING VANILLA RECURRENT NEURAL NETWORK MODEL ######\n",
        "\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model1.fit(X, y, batch_size=128, epochs=40)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "136745/136745 [==============================] - 57s 419us/step - loss: 5.7670 - acc: 0.1245\n",
            "Epoch 2/40\n",
            "136745/136745 [==============================] - 56s 412us/step - loss: 4.9913 - acc: 0.1901\n",
            "Epoch 3/40\n",
            "136745/136745 [==============================] - 57s 418us/step - loss: 4.7305 - acc: 0.2060\n",
            "Epoch 4/40\n",
            "136745/136745 [==============================] - 55s 402us/step - loss: 4.5378 - acc: 0.2192\n",
            "Epoch 5/40\n",
            " 21376/136745 [===>..........................] - ETA: 45s - loss: 4.3710 - acc: 0.2323"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-10393dd3088f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J5ADlReJnz_",
        "colab_type": "text"
      },
      "source": [
        "**Random text of 5 sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUSo_MugJ3iY",
        "colab_type": "code",
        "outputId": "58916714-888f-4c0e-c05b-55c92b2cf9f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "###### LSTM OUTPUT WORDS ######\n",
        "\n",
        "total_score = 0\n",
        "total_count = 0\n",
        "generated_test_lstm = []\n",
        "count = 0\n",
        "\n",
        "for i in range(len(test)):\n",
        "  result = list()\n",
        "  text = clean_doc(test[i])\n",
        "  \n",
        "  if(len(text)>2):\n",
        "    result = text[:seq_length]\n",
        "    r = -1*seq_length\n",
        "    for _ in range(2,len(text)):\n",
        "      encoded = tokenizer.texts_to_sequences([' '.join(result[r:])])[0]\n",
        "      if(len(encoded)<seq_length):\n",
        "        for j in range(len(encoded),seq_length):\n",
        "          encoded.append(0)\n",
        "      encoded = np.array([encoded])\n",
        "      yhat = model.predict_classes(encoded, verbose=0)\n",
        "      out_word = ''\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "        if index == yhat:\n",
        "          out_word = word\n",
        "          break\n",
        "      result.append(out_word)\n",
        "    \n",
        "    generated_test_lstm.append(result)\n",
        "    #print(test[i])\n",
        "\n",
        "    if(count < 5): ### to print 5 sentences only\n",
        "      print(result)\n",
        "    count+=1"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['s', 'but', 'i', 'have', 'a', 'lot', 'of']\n",
            "['s', 'no', 's', 'a', 'lot', 'of', 'the', 'lot', 'of', 'the', 'lot', 'of', 'the']\n",
            "['s', 'youve', 'have', 'a', 'lot', 'of', 'the']\n",
            "['s', 'its', 'the', 'lot', 'of']\n",
            "['s', 'but', 'i', 'have', 'a', 'lot', 'of', 'the', 'lot', 'of', 'the', 'lot', 'of', 'the', 'lot', 'of', 'the', 'lot', 'of', 'the', 'lot', 'of', 'the', 'lot']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sX80AE6ICD7e",
        "outputId": "06835a9f-76e7-47cd-e892-6a73c19b87db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "##### VANILLA RNN OUTPUT WORDS #######\n",
        "\n",
        "total_score = 0\n",
        "total_count = 0\n",
        "generated_test_rnn = []\n",
        "count = 0\n",
        "\n",
        "for i in range(len(test)):\n",
        "  result = list()\n",
        "  text = clean_doc(test[i])\n",
        "  \n",
        "  if(len(text)>2):\n",
        "    result = text[:seq_length]\n",
        "    r = -1*seq_length\n",
        "    for _ in range(2,len(text)):\n",
        "      encoded = tokenizer.texts_to_sequences([' '.join(result[r:])])[0]\n",
        "      if(len(encoded)<seq_length):\n",
        "        for j in range(len(encoded),seq_length):\n",
        "          encoded.append(0)\n",
        "      encoded = np.array([encoded])\n",
        "      yhat = model1.predict_classes(encoded, verbose=0)\n",
        "      out_word = ''\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "        if index == yhat:\n",
        "          out_word = word\n",
        "          break\n",
        "      result.append(out_word)\n",
        "    \n",
        "    generated_test_rnn.append(result)\n",
        "    #print(test[i])\n",
        "\n",
        "    if(count < 5): ### to print 5 sentences only\n",
        "      print(result)\n",
        "    count+=1"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['s', 'but', 'i', 'm', 'going', 'to', 'be']\n",
            "['s', 'no', 'i', 'm', 'going', 'to', 'be', 'a', 'lot', 'of', 'the', 'people', 's']\n",
            "['s', 'youve', 'been', 'going', 'to', 'be', 'a']\n",
            "['s', 'its', 'going', 'to', 'be']\n",
            "['s', 'but', 'i', 'm', 'going', 'to', 'be', 'a', 'lot', 'of', 'the', 'people', 's', 'going', 'to', 'be', 'a', 'lot', 'of', 'the', 'people', 's', 'going', 'to']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfLko1rPCXyu",
        "colab_type": "text"
      },
      "source": [
        "**Calculation of Perplexity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz_blshaBDfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file1 = open(\"/content/drive/My Drive/Theory_of_Computation/speeches.txt\", 'r', encoding=\"utf-8-sig\", errors='ignore')\n",
        "train_1 = file1.read()\n",
        "train_1 = train_1.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAphK3sPBEeX",
        "colab_type": "code",
        "outputId": "62a361b3-795b-4777-fdae-c585dd1c9021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t1 = sent_tokenize(train_1)\n",
        "for i in range (len(t1)):\n",
        "  t1[i] = re.sub(r'[^A-Za-z\\s\\']+', \"\", t1[i])\n",
        "  t1[i] = '<s> '+t1[i]+' </s>'\n",
        "train = t1[:13000] # 80% of dataset is train\n",
        "test = t1[13000:] # 20% of dataset is test\n",
        "print(len(t1))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIw1iObKBFtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "f_1 = {} #bigram\n",
        "for i in range (len(train)):\n",
        "  tokens = train[i].split()\n",
        "  bigrams = nltk.bigrams(tokens)\n",
        "  f = dict(nltk.FreqDist(bigrams))\n",
        "  f_1 = dict(Counter(f)+Counter(f_1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxx7jto7BF6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_1 = re.sub(r'[^A-Za-z\\s\\']+', \"\", train_1)\n",
        "ls = train_1.split()\n",
        "d = {}\n",
        "for i in range (len(ls)):\n",
        "  if ls[i] not in d:\n",
        "    d[ls[i]] = 1\n",
        "  else:\n",
        "    d[ls[i]] += 1\n",
        "Vocab = len(d)\n",
        "Token = sum(d.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG8p0TK4ACzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bigram(w1,w2):\n",
        "  if (w1+\" \"+w2) not in f_1:\n",
        "    if w1 in d.keys():\n",
        "      return 1/(d[w1]+Vocab) # Add-1 Smoothing\n",
        "    else:\n",
        "      return 1/(Vocab) # Add-1 Smoothing\n",
        "  else:\n",
        "    if w1 in d.keys():\n",
        "      return (f_1(w1+\" \"+w2)+1)/(d[w1]+Vocab) # Add-1 Smoothing\n",
        "    else:\n",
        "      return (f_1(w1+\" \"+w2)+1)/(Vocab) # Add-1 Smoothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNtKuoRxAD77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bigram_sentence_probability(sentence):\n",
        "  bigram_sentence_probability_log_sum = 0\n",
        "  previous_word = None\n",
        "  for word in sentence:\n",
        "    if previous_word!=None:\n",
        "      x = bigram(previous_word,word)\n",
        "      bigram_sentence_probability_log_sum += math.log(x,2)\n",
        "    previous_word = word\n",
        "  return math.pow(2, bigram_sentence_probability_log_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPzOwQsLAMpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_number_of_bigrams(sentences):\n",
        "  bigram_count = 0\n",
        "  for sentence in sentences:\n",
        "    bigram_count += len(sentence) - 1\n",
        "  return bigram_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjAwVwb7_KOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_bigram_perplexity(sentences):\n",
        "  bigram_count = calculate_number_of_bigrams(sentences)\n",
        "  sentence_probability_log_sum = 0\n",
        "  for sentence in sentences:\n",
        "    try:\n",
        "      sentence_probability_log_sum -= math.log(bigram_sentence_probability(sentence), 2)\n",
        "    except:\n",
        "      sentence_probability_log_sum -= 0\n",
        "  return math.pow(2, sentence_probability_log_sum / bigram_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV04CPokAlk2",
        "colab_type": "code",
        "outputId": "2b55d59d-375c-46c7-976e-8a7d2599a4a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Perplexity of test corpus with respect to bigram model is:\",end = \" \")\n",
        "print(calc_bigram_perplexity(test))\n",
        "\n",
        "print(\"Perplexity of test corpus generated by LSTM architecture with respect to bigram model is:\",end = \" \")\n",
        "print(calc_bigram_perplexity(generated_test_lstm))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity of test corpus with respect to bigram model is: 147.40810983034604\n",
            "Perplexity of test corpus generated by LSTM architecture with respect to bigram model is: 6096.576391520106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqhfA_tZDrK4",
        "colab_type": "code",
        "outputId": "3ea3dffc-b55d-49ec-9d16-34c6187e8a5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Perplexity of test corpus with respect to bigram model is:\",end = \" \")\n",
        "print(calc_bigram_perplexity(test))\n",
        "\n",
        "print(\"Perplexity of test corpus generated by Baseline RNN architecture with respect to bigram model is:\",end = \" \")\n",
        "print(calc_bigram_perplexity(generated_test_rnn))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity of test corpus with respect to bigram model is: 147.40810983034604\n",
            "Perplexity of test corpus generated by Baseline RNN architecture with respect to bigram model is: 5977.603062968707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRZ7e8lxEFR5",
        "colab_type": "text"
      },
      "source": [
        "# Classical Approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg_Gr2jE8WHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import operator\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARGbaQax8WPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file2 = open(\"/content/drive/My Drive/Theory_of_Computation/speeches.txt\", 'r', encoding=\"utf-8-sig\", errors='ignore')\n",
        "train2 = file2.read()\n",
        "train2 = train2.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEohihYO8WY7",
        "colab_type": "code",
        "outputId": "ed34db1c-cca1-4543-9de2-2a8839e607dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t1 = sent_tokenize(train2)\n",
        "for i in range (len(t1)):\n",
        "  t1[i] = re.sub(r'[^A-Za-z\\s\\']+', \"\", t1[i])\n",
        "  t1[i] = '<s> '+t1[i]+' </s>'\n",
        "train = t1[:13000] \n",
        "test = t1[13000:]\n",
        "print(len(t1))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vwQwi538W3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_1 = {} \n",
        "for i in range (len(train)):\n",
        "  tokens = train[i].split()\n",
        "  bigrams = nltk.bigrams(tokens)\n",
        "  f = dict(nltk.FreqDist(bigrams))\n",
        "  f_1 = dict(Counter(f)+Counter(f_1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HsTMLBn8XjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_2 = {}\n",
        "for i in range (len(train)):\n",
        "  tokens = train[i].split()\n",
        "  trigrams = nltk.trigrams(tokens)\n",
        "  f = dict(nltk.FreqDist(trigrams))\n",
        "  f_2 = dict(Counter(f)+Counter(f_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FYr_Udu8Xu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import everygrams\n",
        "f_3 = {}\n",
        "for i in range (len(train)):\n",
        "  tokens = train[i].split()\n",
        "  fourgrams = list(everygrams(tokens,4,4)) \n",
        "  f = dict(nltk.FreqDist(fourgrams))\n",
        "  f_3 = dict(Counter(f)+Counter(f_3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BgSiufn8X6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train2 = re.sub(r'[^A-Za-z\\s\\']+', \"\", train2)\n",
        "ls = train2.split()\n",
        "d = {}\n",
        "for i in range (len(ls)):\n",
        "  if ls[i] not in d:\n",
        "    d[ls[i]] = 1\n",
        "  else:\n",
        "    d[ls[i]] += 1\n",
        "Vocab = len(d)\n",
        "Token = sum(d.values())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1g6UBzi9TZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unigram(w1):\n",
        "  if w1 not in d:\n",
        "    return 1/(Vocab+Token) # Add-1 Smoothing\n",
        "  else:\n",
        "    return (d[w1]+1)/(Vocab+Token) # Add-1 Smoothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MHXDJjq9U_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bigram(w1,w2):\n",
        "  if (w1+\" \"+w2) not in f_1:\n",
        "    if w1 in d.keys():\n",
        "      return 1/(d[w1]+Vocab) # Add-1 Smoothing\n",
        "    else:\n",
        "      return 1/(Vocab) # Add-1 Smoothing\n",
        "  else:\n",
        "    if w1 in d.keys():\n",
        "      return (f_1(w1+\" \"+w2)+1)/(d[w1]+Vocab) # Add-1 Smoothing\n",
        "    else:\n",
        "      return (f_1(w1+\" \"+w2)+1)/(Vocab) # Add-1 Smoothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcDrdpVq9WtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trigram(w1,w2,w3):\n",
        "  if (w1+\" \"+w2+\" \"+w3) not in f_2:\n",
        "    if w1+\" \"+w2 in f_1.keys():\n",
        "      return 1/(f_1[w1+\" \"+w2]+Vocab) # Add-1 Smoothing\n",
        "    else:\n",
        "      return 1/(Vocab) # Add-1 Smoothing\n",
        "  else:\n",
        "    if w1+\" \"+w2 in f_1.keys():\n",
        "      return (f_2(w1+\" \"+w2+\" \"+w3)+1)/(fdist1[w1+\" \"+w2]+Vocab) # Add-1 Smoothing\n",
        "    else:\n",
        "      return (f_2(w1+\" \"+w2+\" \"+w3)+1)/(Vocab) # Add-1 Smoothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0W4Cm4nd9YfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def quadgram(w1,w2,w3,w4):\n",
        "  if (w1+\" \"+w2+\" \"+w3+\" \"+w4) not in f_3:\n",
        "    if w1+\" \"+w2 + \" \"+w3 in f_2.keys():\n",
        "      return 1/(f_2[w1+\" \"+w2+\" \"+w3]+Vocab) # Add-1 Smoothing\n",
        "    else:\n",
        "      return 1/(Vocab) # Add-1 Smoothing\n",
        "  else:\n",
        "    if w1+\" \"+w2 + \" \"+w3 in f_2.keys():\n",
        "      return (f_3(w1+\" \"+w2+\" \"+w3+\" \"+w4)+1)/(f_2[w1+\" \"+w2+\" \"+w3]+Vocab) # Add-1 Smoothing\n",
        "    else:\n",
        "      return (f_3(w1+\" \"+w2+\" \"+w3+\" \"+w4)+1)/(Vocab) # Add-1 Smoothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJpFtrbk9acC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unigram_sentence_probability(sentence):\n",
        "  sentence_probability_log_sum = 0\n",
        "  for word in sentence:\n",
        "    x = unigram(word)\n",
        "    sentence_probability_log_sum += math.log(x,2)\n",
        "  return math.pow(2, sentence_probability_log_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfXurj_d9cKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bigram_sentence_probability(sentence):\n",
        "  bigram_sentence_probability_log_sum = 0\n",
        "  previous_word = None\n",
        "  for word in sentence:\n",
        "    if previous_word!=None:\n",
        "      x = bigram(previous_word,word)\n",
        "      bigram_sentence_probability_log_sum += math.log(x,2)\n",
        "    previous_word = word\n",
        "  return math.pow(2, bigram_sentence_probability_log_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-vznvtP9d7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trigram_sentence_probability(sentence):\n",
        "  trigram_sentence_probability_log_sum = 0\n",
        "  previous_word = None\n",
        "  previous_previous_word = None\n",
        "  for word in sentence:\n",
        "    if previous_word!=None and previous_previous_word!=None:\n",
        "      x = trigram(previous_previous_word,previous_word,word)\n",
        "      trigram_sentence_probability_log_sum += math.log(x,2)\n",
        "    previous_previous_word = previous_word\n",
        "    previous_word = word\n",
        "  return math.pow(2, trigram_sentence_probability_log_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3CfMNvL9gHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def quadgram_sentence_probability(sentence):\n",
        "  quadgram_sentence_probability_log_sum = 0\n",
        "  previous_word = None\n",
        "  previous_previous_word = None\n",
        "  previous_previous_previous_word = None\n",
        "  for word in sentence:\n",
        "    if previous_word!=None and previous_previous_word!=None and previous_previous_previous_word!=None :\n",
        "      x = quadgram(previous_previous_previous_previous_previous_word,previous_word,word)\n",
        "      quadgram_sentence_probability_log_sum += math.log(x,2)\n",
        "    previous_previous_previous_word = previous_previous_word\n",
        "    previous_previous_word = previous_word\n",
        "    previous_word = word\n",
        "  return math.pow(2, quadgram_sentence_probability_log_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTKYMCgk9iCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_number_of_unigrams(sentences):\n",
        "  unigram_count = 0\n",
        "  for sentence in sentences:\n",
        "    # remove two for <s> and </s>\n",
        "    unigram_count += len(sentence) - 2\n",
        "  return unigram_count\n",
        "def calculate_number_of_bigrams(sentences):\n",
        "  bigram_count = 0\n",
        "  for sentence in sentences:\n",
        "    bigram_count += len(sentence) - 1\n",
        "  return bigram_count\n",
        "def calculate_number_of_trigrams(sentences):\n",
        "  trigram_count = 0\n",
        "  for sentence in sentences:\n",
        "    trigram_count += len(sentence) - 2\n",
        "  return trigram_count\n",
        "def calculate_number_of_quadgrams(sentences):\n",
        "  quadgram_count = 0\n",
        "  for sentence in sentences:\n",
        "    quadgram_count += len(sentence) - 3\n",
        "  return quadgram_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VyVDwr19j68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_unigram_perplexity(sentences):\n",
        "  unigram_count = calculate_number_of_unigrams(sentences)\n",
        "  sentence_probability_log_sum = 0\n",
        "  for sentence in sentences:\n",
        "    try:\n",
        "      sentence_probability_log_sum -= math.log(unigram_sentence_probability(sentence), 2)\n",
        "    except:\n",
        "      sentence_probability_log_sum -= 0\n",
        "  return math.pow(2, sentence_probability_log_sum / unigram_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNkBPs4E9l8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_bigram_perplexity(sentences):\n",
        "  bigram_count = calculate_number_of_bigrams(sentences)\n",
        "  sentence_probability_log_sum = 0\n",
        "  for sentence in sentences:\n",
        "    try:\n",
        "      sentence_probability_log_sum -= math.log(bigram_sentence_probability(sentence), 2)\n",
        "    except:\n",
        "      sentence_probability_log_sum -= 0\n",
        "  return math.pow(2, sentence_probability_log_sum / bigram_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcFMT6Po9n0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_trigram_perplexity(sentences):\n",
        "  trigram_count = calculate_number_of_bigrams(sentences)\n",
        "  sentence_probability_log_sum = 0\n",
        "  for sentence in sentences:\n",
        "    try:\n",
        "      sentence_probability_log_sum -= math.log(trigram_sentence_probability(sentence), 2)\n",
        "    except:\n",
        "      sentence_probability_log_sum -= 0\n",
        "  return math.pow(2, sentence_probability_log_sum / trigram_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d6d9iOHbJMOn",
        "colab": {}
      },
      "source": [
        "def calc_quadgram_perplexity(sentences):\n",
        "  quadgram_count = calculate_number_of_quadgrams(sentences)\n",
        "  sentence_probability_log_sum = 0\n",
        "  for sentence in sentences:\n",
        "    try:\n",
        "      sentence_probability_log_sum -= math.log(quadgram_sentence_probability(sentence), 2)\n",
        "    except:\n",
        "      sentence_probability_log_sum -= 0\n",
        "  return math.pow(2, sentence_probability_log_sum / quadgram_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ7kHZHuJOR5",
        "colab_type": "text"
      },
      "source": [
        "**Calculation of Perplexity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDAOgijI9rRw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b0c545b5-c905-4a21-a53d-ecb3e77314c7"
      },
      "source": [
        "import math\n",
        "print(\"Perplexity of test corpus with respect to unigram model is:\",end = \" \")\n",
        "print(calc_unigram_perplexity(test))\n",
        "print(\"Perplexity of test corpus with respect to bigram model is:\",end = \" \")\n",
        "print(calc_bigram_perplexity(test))\n",
        "print(\"Perplexity of test corpus with respect to trigram model is:\",end = \" \")\n",
        "print(calc_trigram_perplexity(test))\n",
        "print(\"Perplexity of test corpus with respect to quadgram model is:\",end = \" \")\n",
        "print(calc_quadgram_perplexity(test))\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity of test corpus with respect to unigram model is: 186.64101133266124\n",
            "Perplexity of test corpus with respect to bigram model is: 147.40810983034604\n",
            "Perplexity of test corpus with respect to trigram model is: 135.92047675682477\n",
            "Perplexity of test corpus with respect to quadgram model is: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfeb1PgK9tZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Classical generator to generate new words using a n_gram model\n",
        "def mle_generator(n_gram, initial_sequence):\n",
        "  sentence = [\"<s>\"]\n",
        "  if(n_gram==1):\n",
        "    for i in range(20):\n",
        "      max_prob = 0\n",
        "      max_prob_list = list()\n",
        "      for j in d.keys():\n",
        "        k = (unigram(j))\n",
        "        if(k>max_prob):\n",
        "          max_prob = k\n",
        "          max_prob_list = [j]\n",
        "        elif(k==max_prob):\n",
        "          max_prob_list.append(j)\n",
        "      samples = np.random.multinomial(20,[max_prob]*len(max_prob_list),size=1)\n",
        "      index, value = max(enumerate(samples), key = operator.itemgetter(1))\n",
        "      sentence.append(max_prob_list[index])\n",
        "  else:\n",
        "    sentence.extend(initial_sequence)\n",
        "    i = len(initial_sequence)\n",
        "    while(sentence[-1]!=\"</s>\" and i<20):\n",
        "      max_prob = 0\n",
        "      max_prob_list = list()\n",
        "      for j in d.keys():\n",
        "        word_list = sentence[-n_gram+1:]\n",
        "        word_list.append(j)\n",
        "        if(n_gram==2):\n",
        "          k = bigram(word_list[0],word_list[1])\n",
        "        elif(n_gram==3):\n",
        "          k = trigram(word_list[0],word_list[1],word_list[2])\n",
        "        elif(n_gram==4):\n",
        "          k = quadgram(word_list[0],word_list[1],word_list[2],word_list[3])\n",
        "        if(k>max_prob):\n",
        "          max_prob = k\n",
        "          max_prob_list = [j]\n",
        "        elif(k==max_prob):\n",
        "          max_prob_list.append(j)\n",
        "      samples = np.random.multinomial(20,[max_prob]*len(max_prob_list),size=1)\n",
        "      index, value = max(enumerate(samples), key = operator.itemgetter(1))\n",
        "      sentence.append(max_prob_list[index])\n",
        "      i+=1\n",
        "    \n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3F6UszC9wso",
        "colab_type": "code",
        "outputId": "0d0c94b2-21c8-4a52-bb88-0949e2891d9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\" \".join(mle_generator(3,[\"it\", \"should\", \"not\"])))\n"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<s> it should not speech speech speech speech speech speech speech speech speech speech speech speech speech speech speech speech speech\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO0zjY-BJT0o",
        "colab_type": "text"
      },
      "source": [
        "**Random text generation of 5 sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuK3MHYY9Imj",
        "colab_type": "code",
        "outputId": "c567492c-39c9-43b9-ce31-a7fb76322365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# GENERATING RANDOM TEXT:\n",
        "# UNIGRAMS\n",
        "print(\"UNIGRAM: \", end = \" \")\n",
        "print(\" \".join(MLE_Generator(1,[])))\n",
        "# BIGRAM\n",
        "print(\"BIGRAM: \", end = \" \")\n",
        "print(\" \".join(MLE_Generator(2,[\"it\", \"is\"])))\n",
        "# TRIGRAM\n",
        "print(\"TRIGRAM: \", end = \" \")\n",
        "print(\" \".join(MLE_Generator(3,[\"there\", \"is\", \"something\"])))\n",
        "# QUADGRAM\n",
        "print(\"QUADGRAM1: \", end = \" \")\n",
        "print(\" \".join(MLE_Generator(4,[\"but\", \"there\", \"is\", \"no\"])))\n",
        "# ANOTHER QuADGRAM\n",
        "print(\"QUADGRAM2: \", end = \" \")\n",
        "print(\" \".join(MLE_Generator(4,[\"it\", \"is\", \"to\", \"be\"])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UNIGRAM:  <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s> <s>\n",
            "BIGRAM:  <s> it is a lot of the world </s>\n",
            "TRIGRAM:  <s> there is something that i can tell you that </s>\n",
            "QUADGRAM1:  <s> but there is no leadership at the top </s>\n",
            "QUADGRAM2:  <s> it is to be treated a little bit </s>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}